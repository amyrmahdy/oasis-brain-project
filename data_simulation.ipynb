{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Nv6kc4-JHCKg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random \n",
        "import numpy.random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import scipy.special\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DfLXv-9TH42S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uF1gd90XHCKn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "96179e1b-38c0-426e-d1d4-0bfcb88798d3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b27cb863a974>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morginal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/abc.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmu_eTIV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_eTIV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morginal_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eTIV'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morginal_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eTIV'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmu_MMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_MMSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morginal_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MMSE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morginal_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MMSE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmu_EDUC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_EDUC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morginal_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EDUC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morginal_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EDUC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mage_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/abc.csv'"
          ]
        }
      ],
      "source": [
        "orginal_data = pd.read_csv(\"/content/abc.csv\")\n",
        "mu_eTIV, sigma_eTIV = orginal_data['eTIV'].mean(), orginal_data['eTIV'].var()\n",
        "mu_MMSE, sigma_MMSE = orginal_data['MMSE'].mean(), orginal_data['MMSE'].var()\n",
        "mu_EDUC, sigma_EDUC = orginal_data['EDUC'].mean(), orginal_data['EDUC'].var()\n",
        "age_range = (60, 100)\n",
        "\n",
        "def build_lstm_rnn(input_shape):\n",
        "    lstm_rnn = tf.keras.Sequential()\n",
        "    lstm_rnn.add(tf.keras.layers.LSTM(100, return_sequences=True, input_shape=input_shape))\n",
        "    lstm_rnn.add(tf.keras.layers.LSTM(50, return_sequences=False))\n",
        "    lstm_rnn.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "    lstm_rnn.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "    return lstm_rnn\n",
        "\n",
        "accuracy_list = []\n",
        "recall_list = []\n",
        "precision_list = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = eta = np.zeros((N, m))\n",
        "Age = np.random.choice(range(age_range[0], age_range[1] + 1), size=N)\n",
        "MMSE = np.round(np.random.normal(mu_MMSE, sigma_MMSE, size=N))\n",
        "EDUC = np.round(np.random.normal(mu_EDUC, sigma_EDUC, size=N))\n",
        "Sex = np.random.binomial(1, 0.2, size=N)\n",
        "eTIV = np.round(np.random.normal(mu_eTIV, sigma_eTIV, size=N))\n",
        "ASF = np.random.normal(size=N)\n",
        "nWBV = np.random.normal(size=N)\n",
        "Visit = np.round(np.linspace(0, 4, num=m))\n",
        "\n",
        "B = np.array([0.1, -0.1, 0.2, -0.2, 0.3, 0.3, 0.1, 0.1, 0.2, 0.1])\n",
        "b = np.random.normal(0, 0.5, size=N)\n",
        "for i in range(N):\n",
        "    for j in range(m):\n",
        "            eta[i, j] = B[0] + B[1] * Visit[j] + B[2] * Age[i] + B[3] * MMSE[i] + B[4] * EDUC[i] + B[5] * Sex[i] + \\\n",
        "                        B[6] * eTIV[i] + B[7] * ASF[i] + B[8] * nWBV[i] + b[i]\n",
        "            p = scipy.special.expit(eta[i, j])\n",
        "            Y[i, j] = np.random.binomial(1, p)\n",
        "\n",
        "   \n",
        "\n",
        "    mat = np.empty((N * m, 10))\n",
        "    p = 0\n",
        "    for i in range(N):\n",
        "        jk = 0\n",
        "        for jj in range(p, p + m):\n",
        "            mat[jj, 0] = i + 1\n",
        "            mat[jj, 1] = Visit[jk]\n",
        "            mat[jj, 2] = Age[i]\n",
        "            mat[jj, 3] = MMSE[i]\n",
        "            mat[jj, 4] = EDUC[i]\n",
        "            mat[jj, 5] = Sex[i]\n",
        "            mat[jj, 6] = eTIV[jk]\n",
        "            mat[jj, 7] = ASF[jk]\n",
        "            mat[jj, 8] = nWBV[jk]\n",
        "            mat[jj, 9] = Y[i, jk]\n",
        "            jk += 1\n",
        "        p += m\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "Fik2FHsQH50b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the `mat` array to a DataFrame\n",
        "df_mat = pd.DataFrame(mat, columns=['ID', 'Visit', 'Age', 'MMSE', 'EDUC', 'Sex', 'eTIV', 'ASF', 'nWBV', 'Y'])\n",
        "df = pd.get_dummies(df_mat, columns=[\"Visit\"], prefix=\"Visit\")\n",
        "RANDOM_STATE = 13\n",
        "splitter = GroupShuffleSplit(test_size=.20, n_splits=1, random_state=RANDOM_STATE)\n",
        "split = splitter.split(df, groups=df['ID'])\n",
        "train_indexes, test_indexes = next(split)\n",
        "\n",
        "X_train = df.iloc[train_indexes]\n",
        "X_test = df.iloc[test_indexes]\n",
        "age_scaler = MinMaxScaler()\n",
        "educ_scaler = MinMaxScaler()\n",
        "mmse_scaler = MinMaxScaler()\n",
        "etiv_scaler = MinMaxScaler()\n",
        "\n",
        "age_scaler.fit(X_train[\"Age\"].to_numpy().reshape(-1, 1))\n",
        "educ_scaler.fit(X_train[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
        "mmse_scaler.fit(X_train[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
        "etiv_scaler.fit(X_train[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
        "\n",
        "X_train[\"Age\"] = age_scaler.transform(X_train[\"Age\"].to_numpy().reshape(-1, 1))\n",
        "X_train[\"EDUC\"] = educ_scaler.transform(X_train[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
        "X_train[\"MMSE\"] = mmse_scaler.transform(X_train[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
        "X_train[\"eTIV\"] = etiv_scaler.transform(X_train[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
        "\n",
        "X_test[\"Age\"] = age_scaler.transform(X_test[\"Age\"].to_numpy().reshape(-1, 1))\n",
        "X_test[\"EDUC\"] = educ_scaler.transform(X_test[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
        "X_test[\"MMSE\"] = mmse_scaler.transform(X_test[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
        "X_test[\"eTIV\"] = mmse_scaler.transform(X_test[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
        "visit_3 = df[df[\"Visit_4.0\"] == 1][[\"ID\", \"Y\"]]\n",
        "y_train_final = []\n",
        "for id in X_train[\"ID\"]:\n",
        "        y_train_final.append(visit_3[visit_3[\"ID\"] == id][\"Y\"].values[0])\n",
        "\n",
        "y_test_final = []\n",
        "for id in X_test[\"ID\"]:\n",
        "        y_test_final.append(visit_3[visit_3[\"ID\"] == id][\"Y\"].values[0])\n",
        "\n",
        "y_train_super_final = pd.DataFrame({\n",
        "\"Group\": y_train_final})\n",
        "y_test_super_final = pd.DataFrame({\n",
        "\"Group\": y_test_final})\n",
        "X_test_final = X_test.drop([\"ID\"], axis = 1)\n",
        "X_train_final = X_train.drop([\"ID\"], axis = 1)\n",
        "rows_n = 5\n",
        "\n",
        "X_train_super_final = np.reshape(X_train_final.to_numpy(),(X_train_final.shape[0]//5,X_train_final.shape[1],rows_n))\n",
        "X_test_super_final = np.reshape(X_test_final.to_numpy(),(X_test_final.shape[0]//5,X_train_final.shape[1],rows_n))\n",
        "lstm_rnn = build_lstm_rnn((X_train_super_final.shape[1],X_train_super_final.shape[2]) )\n",
        "lstm_rnn.fit(X_train_super_final, y_train_super_final[0:int(len(X_train_super_final))], epochs = 100)\n",
        "not_final_lstm_rnn_prediction = lstm_rnn.predict(X_test_super_final)\n",
        "lstm_rnn_prediction = np.where(not_final_lstm_rnn_prediction > 0.5, 1, 0)\n",
        "lstm_rnn_conf = confusion_matrix(y_test_super_final[0:int(len(lstm_rnn_prediction))],lstm_rnn_prediction)\n",
        "lstm_rnn_plot_conf = ConfusionMatrixDisplay(lstm_rnn_conf)\n",
        "lstm_rnn_plot_conf.plot()\n",
        "TP=lstm_rnn_conf[1,1]\n",
        "FP=lstm_rnn_conf[0,1]\n",
        "TN=lstm_rnn_conf[0,0]\n",
        "FN=lstm_rnn_conf[1,0]\n",
        "accuracy=(TP+TN)/(TP+FP+FN+TN)\n",
        "recall =TP/(TP+FN)\n",
        "precision =TP/(TP+TN)\n",
        "accuracy_list.append(accuracy)\n",
        "recall_list.append(recall)\n",
        "precision_list.append(precision)"
      ],
      "metadata": {
        "id": "QTaa5YTlIPAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XczazMdmHCKo"
      },
      "outputs": [],
      "source": [
        "for _ in range(10):\n",
        "    N = 100\n",
        "    m = 5\n",
        "\n",
        "    Y = eta = np.zeros((N, m))\n",
        "    Age = np.random.choice(range(age_range[0], age_range[1] + 1), size=N)\n",
        "    MMSE = np.round(np.random.normal(mu_MMSE, sigma_MMSE, size=N))\n",
        "    EDUC = np.round(np.random.normal(mu_EDUC, sigma_EDUC, size=N))\n",
        "    Sex = np.random.binomial(1, 0.2, size=N)\n",
        "    eTIV = np.round(np.random.normal(mu_eTIV, sigma_eTIV, size=N))\n",
        "    ASF = np.random.normal(size=N)\n",
        "    nWBV = np.random.normal(size=N)\n",
        "    Visit = np.round(np.linspace(0, 4, num=m))\n",
        "\n",
        "    B = np.array([0.1, -0.1, 0.2, -0.2, 0.3, 0.3, 0.1, 0.1, 0.2, 0.1])\n",
        "    b = np.random.normal(0, 0.5, size=N)\n",
        "\n",
        "    for i in range(N):\n",
        "        for j in range(m):\n",
        "            eta[i, j] = B[0] + B[1] * Visit[j] + B[2] * Age[i] + B[3] * MMSE[i] + B[4] * EDUC[i] + B[5] * Sex[i] + \\\n",
        "                        B[6] * eTIV[i] + B[7] * ASF[i] + B[8] * nWBV[i] + b[i]\n",
        "            p = scipy.special.expit(eta[i, j])\n",
        "            Y[i, j] = np.random.binomial(1, p)\n",
        "\n",
        "   \n",
        "\n",
        "    mat = np.empty((N * m, 10))\n",
        "    p = 0\n",
        "    for i in range(N):\n",
        "        jk = 0\n",
        "        for jj in range(p, p + m):\n",
        "            mat[jj, 0] = i + 1\n",
        "            mat[jj, 1] = Visit[jk]\n",
        "            mat[jj, 2] = Age[i]\n",
        "            mat[jj, 3] = MMSE[i]\n",
        "            mat[jj, 4] = EDUC[i]\n",
        "            mat[jj, 5] = Sex[i]\n",
        "            mat[jj, 6] = eTIV[jk]\n",
        "            mat[jj, 7] = ASF[jk]\n",
        "            mat[jj, 8] = nWBV[jk]\n",
        "            mat[jj, 9] = Y[i, jk]\n",
        "            jk += 1\n",
        "        p += m\n",
        "\n",
        "    # Convert the `mat` array to a DataFrame\n",
        "    df_mat = pd.DataFrame(mat, columns=['ID', 'Visit', 'Age', 'MMSE', 'EDUC', 'Sex', 'eTIV', 'ASF', 'nWBV', 'Y'])\n",
        "    df = pd.get_dummies(df_mat, columns=[\"Visit\"], prefix=\"Visit\")\n",
        "    RANDOM_STATE = 13\n",
        "    splitter = GroupShuffleSplit(test_size=.20, n_splits=1, random_state=RANDOM_STATE)\n",
        "    split = splitter.split(df, groups=df['ID'])\n",
        "    train_indexes, test_indexes = next(split)\n",
        "\n",
        "    X_train = df.iloc[train_indexes]\n",
        "    X_test = df.iloc[test_indexes]\n",
        "    age_scaler = MinMaxScaler()\n",
        "    educ_scaler = MinMaxScaler()\n",
        "    mmse_scaler = MinMaxScaler()\n",
        "    etiv_scaler = MinMaxScaler()\n",
        "\n",
        "    age_scaler.fit(X_train[\"Age\"].to_numpy().reshape(-1, 1))\n",
        "    educ_scaler.fit(X_train[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
        "    mmse_scaler.fit(X_train[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
        "    etiv_scaler.fit(X_train[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
        "\n",
        "    X_train[\"Age\"] = age_scaler.transform(X_train[\"Age\"].to_numpy().reshape(-1, 1))\n",
        "    X_train[\"EDUC\"] = educ_scaler.transform(X_train[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
        "    X_train[\"MMSE\"] = mmse_scaler.transform(X_train[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
        "    X_train[\"eTIV\"] = etiv_scaler.transform(X_train[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
        "\n",
        "    X_test[\"Age\"] = age_scaler.transform(X_test[\"Age\"].to_numpy().reshape(-1, 1))\n",
        "    X_test[\"EDUC\"] = educ_scaler.transform(X_test[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
        "    X_test[\"MMSE\"] = mmse_scaler.transform(X_test[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
        "    X_test[\"eTIV\"] = mmse_scaler.transform(X_test[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
        "    visit_3 = df[df[\"Visit_4.0\"] == 1][[\"ID\", \"Y\"]]\n",
        "    y_train_final = []\n",
        "    for id in X_train[\"ID\"]:\n",
        "        y_train_final.append(visit_3[visit_3[\"ID\"] == id][\"Y\"].values[0])\n",
        "\n",
        "    y_test_final = []\n",
        "    for id in X_test[\"ID\"]:\n",
        "        y_test_final.append(visit_3[visit_3[\"ID\"] == id][\"Y\"].values[0])\n",
        "\n",
        "    y_train_super_final = pd.DataFrame({\n",
        "    \"Group\": y_train_final})\n",
        "    y_test_super_final = pd.DataFrame({\n",
        "    \"Group\": y_test_final})\n",
        "    X_test_final = X_test.drop([\"ID\"], axis = 1)\n",
        "    X_train_final = X_train.drop([\"ID\"], axis = 1)\n",
        "    rows_n = 5\n",
        "\n",
        "    X_train_super_final = np.reshape(X_train_final.to_numpy(),(X_train_final.shape[0]//5,X_train_final.shape[1],rows_n))\n",
        "    X_test_super_final = np.reshape(X_test_final.to_numpy(),(X_test_final.shape[0]//5,X_train_final.shape[1],rows_n))\n",
        "    lstm_rnn = build_lstm_rnn((X_train_super_final.shape[1],X_train_super_final.shape[2]) )\n",
        "    lstm_rnn.fit(X_train_super_final, y_train_super_final[0:int(len(X_train_super_final))], epochs = 100)\n",
        "    not_final_lstm_rnn_prediction = lstm_rnn.predict(X_test_super_final)\n",
        "    lstm_rnn_prediction = np.where(not_final_lstm_rnn_prediction > 0.5, 1, 0)\n",
        "    lstm_rnn_conf = confusion_matrix(y_test_super_final[0:int(len(lstm_rnn_prediction))],lstm_rnn_prediction)\n",
        "    lstm_rnn_plot_conf = ConfusionMatrixDisplay(lstm_rnn_conf)\n",
        "    lstm_rnn_plot_conf.plot()\n",
        "    TP=lstm_rnn_conf[1,1]\n",
        "    FP=lstm_rnn_conf[0,1]\n",
        "    TN=lstm_rnn_conf[0,0]\n",
        "    FN=lstm_rnn_conf[1,0]\n",
        "    accuracy=(TP+TN)/(TP+FP+FN+TN)\n",
        "    recall =TP/(TP+FN)\n",
        "    precision =TP/(TP+TN)\n",
        "    accuracy_list.append(accuracy)\n",
        "    recall_list.append(recall)\n",
        "    precision_list.append(precision)\n",
        "\n",
        "print(\"Accuracy List:\", accuracy_list)\n",
        "print(\"Recall List:\", recall_list)\n",
        "print(\"Precision List:\", precision_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n"
      ],
      "metadata": {
        "id": "p7Hdymx9MeeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I8iwnh4LMfmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdmxTuQ5HCKr"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy List:\", accuracy_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Recall List:\", recall_list)\n"
      ],
      "metadata": {
        "id": "Qi-vd_pCLQUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Precision List:\", precision_list)"
      ],
      "metadata": {
        "id": "UZz1mSZRLSGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_accuracy_list = statistics.mean(accuracy_list)\n",
        "mean_recall_list = statistics.mean(recall_list)\n",
        "mean_precision_list = statistics.mean(precision_list)\n",
        "print(\"mean_accuracy:\",mean_accuracy_list, \"mean_recall:\",mean_recall_list,\"mean_precision:\",mean_precision_list)"
      ],
      "metadata": {
        "id": "8pA9Ui0RMkQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    inputshape=(X_train_super_final.shape[1],X_train_super_final.shape[2])\n"
      ],
      "metadata": {
        "id": "yBxSKdyEPPYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gru_rnn(input_shape):\n",
        "    gru_rnn = tf.keras.Sequential()\n",
        "    gru_rnn.add(tf.keras.layers.GRU(100, return_sequences = True,input_shape = True) )\n",
        "    gru_rnn.add(tf.keras.layers.GRU(50,return_sequences = False))\n",
        "    gru_rnn.add(tf.keras.layers.Dense(1,activation = \"sigmoid\"))\n",
        "    gru_rnn.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "    return gru_rnn"
      ],
      "metadata": {
        "id": "0yaMUJUFNqbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "    N = 100\n",
        "    m = 5\n",
        "\n",
        "    Y = eta = np.zeros((N, m))\n",
        "    Age = np.random.choice(range(age_range[0], age_range[1] + 1), size=N)\n",
        "    MMSE = np.round(np.random.normal(mu_MMSE, sigma_MMSE, size=N))\n",
        "    EDUC = np.round(np.random.normal(mu_EDUC, sigma_EDUC, size=N))\n",
        "    Sex = np.random.binomial(1, 0.2, size=N)\n",
        "    eTIV = np.round(np.random.normal(mu_eTIV, sigma_eTIV, size=N))\n",
        "    ASF = np.random.normal(size=N)\n",
        "    nWBV = np.random.normal(size=N)\n",
        "    Visit = np.round(np.linspace(0, 4, num=m))\n",
        "\n",
        "    B = np.array([0.1, -0.1, 0.2, -0.2, 0.3, 0.3, 0.1, 0.1, 0.2, 0.1])\n",
        "    b = np.random.normal(0, 0.5, size=N)\n",
        "\n",
        "    for i in range(N):\n",
        "        for j in range(m):\n",
        "            eta[i, j] = B[0] + B[1] * Visit[j] + B[2] * Age[i] + B[3] * MMSE[i] + B[4] * EDUC[i] + B[5] * Sex[i] + \\\n",
        "                        B[6] * eTIV[i] + B[7] * ASF[i] + B[8] * nWBV[i] + b[i]\n",
        "            p = scipy.special.expit(eta[i, j])\n",
        "            Y[i, j] = np.random.binomial(1, p)\n",
        "\n",
        "   \n",
        "\n",
        "    mat = np.empty((N * m, 10))\n",
        "    p = 0\n",
        "    for i in range(N):\n",
        "        jk = 0\n",
        "        for jj in range(p, p + m):\n",
        "            mat[jj, 0] = i + 1\n",
        "            mat[jj, 1] = Visit[jk]\n",
        "            mat[jj, 2] = Age[i]\n",
        "            mat[jj, 3] = MMSE[i]\n",
        "            mat[jj, 4] = EDUC[i]\n",
        "            mat[jj, 5] = Sex[i]\n",
        "            mat[jj, 6] = eTIV[jk]\n",
        "            mat[jj, 7] = ASF[jk]\n",
        "            mat[jj, 8] = nWBV[jk]\n",
        "            mat[jj, 9] = Y[i, jk]\n",
        "            jk += 1\n",
        "        p += m\n",
        "\n",
        "    # Convert the `mat` array to a DataFrame\n",
        "    df_mat = pd.DataFrame(mat, columns=['ID', 'Visit', 'Age', 'MMSE', 'EDUC', 'Sex', 'eTIV', 'ASF', 'nWBV', 'Y'])\n",
        "    df = pd.get_dummies(df_mat, columns=[\"Visit\"], prefix=\"Visit\")\n",
        "    RANDOM_STATE = 13\n",
        "    splitter = GroupShuffleSplit(test_size=.20, n_splits=1, random_state=RANDOM_STATE)\n",
        "    split = splitter.split(df, groups=df['ID'])\n",
        "    train_indexes, test_indexes = next(split)\n",
        "\n",
        "    X_train = df.iloc[train_indexes]\n",
        "    X_test = df.iloc[test_indexes]\n",
        "    age_scaler = MinMaxScaler()\n",
        "    educ_scaler = MinMaxScaler()\n",
        "    mmse_scaler = MinMaxScaler()\n",
        "    etiv_scaler = MinMaxScaler()\n",
        "\n",
        "    age_scaler.fit(X_train[\"Age\"].to_numpy().reshape(-1, 1))\n",
        "    educ_scaler.fit(X_train[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
        "    mmse_scaler.fit(X_train[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
        "    etiv_scaler.fit(X_train[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
        "\n",
        "    X_train[\"Age\"] = age_scaler.transform(X_train[\"Age\"].to_numpy().reshape(-1, 1))\n",
        "    X_train[\"EDUC\"] = educ_scaler.transform(X_train[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
        "    X_train[\"MMSE\"] = mmse_scaler.transform(X_train[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
        "    X_train[\"eTIV\"] = etiv_scaler.transform(X_train[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
        "\n",
        "    X_test[\"Age\"] = age_scaler.transform(X_test[\"Age\"].to_numpy().reshape(-1, 1))\n",
        "    X_test[\"EDUC\"] = educ_scaler.transform(X_test[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
        "    X_test[\"MMSE\"] = mmse_scaler.transform(X_test[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
        "    X_test[\"eTIV\"] = mmse_scaler.transform(X_test[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
        "    visit_3 = df[df[\"Visit_4.0\"] == 1][[\"ID\", \"Y\"]]\n",
        "    y_train_final = []\n",
        "    for id in X_train[\"ID\"]:\n",
        "        y_train_final.append(visit_3[visit_3[\"ID\"] == id][\"Y\"].values[0])\n",
        "\n",
        "    y_test_final = []\n",
        "    for id in X_test[\"ID\"]:\n",
        "        y_test_final.append(visit_3[visit_3[\"ID\"] == id][\"Y\"].values[0])\n",
        "\n",
        "    y_train_super_final = pd.DataFrame({\n",
        "    \"Group\": y_train_final})\n",
        "    y_test_super_final = pd.DataFrame({\n",
        "    \"Group\": y_test_final})\n",
        "    X_test_final = X_test.drop([\"ID\"], axis = 1)\n",
        "    X_train_final = X_train.drop([\"ID\"], axis = 1)\n",
        "    rows_n = 5"
      ],
      "metadata": {
        "id": "pVp2GE-DjA7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    X_train_super_final = np.reshape(X_train_final.to_numpy(),(X_train_final.shape[0]//5,X_train_final.shape[1],rows_n))\n",
        "    X_test_super_final = np.reshape(X_test_final.to_numpy(),(X_test_final.shape[0]//5,X_train_final.shape[1],rows_n))"
      ],
      "metadata": {
        "id": "RaLk1r7tjK8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_super_final.shape[1]"
      ],
      "metadata": {
        "id": "FKBHDKm6jemY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_rnn = build_gru_rnn((X_train_super_final.shape[1],X_train_super_final.shape[2]))\n",
        "gru_rnn.fit(X_train_super_final, y_train_super_final[0:int(len(X_train_super_final))], epochs = 100)\n",
        "not_final_gru_rnn_prediction = gru_rnn.predict(X_test_super_final)\n",
        "gru_rnn_prediction = np.where(not_final_gru_rnn_prediction > 0.5, 1, 0)\n",
        "print(classification_report(y_test_super_final,gru_rnn_prediction, target_names= [\"Non-Demanted\", \"Demanted\"]))\n",
        "gru_rnn_conf = confusion_matrix(y_test_super_final[0:int(len(lstm_rnn_prediction))],gru_rnn_prediction)\n",
        "gru_rnn_plot_conf = ConfusionMatrixDisplay(gru_rnn_conf)\n",
        "gru_rnn_plot_conf.plot()"
      ],
      "metadata": {
        "id": "JLk9XZQ9jQy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "    N = 100\n",
        "    m = 5\n",
        "\n",
        "    Y = eta = np.zeros((N, m))\n",
        "    Age = np.random.choice(range(age_range[0], age_range[1] + 1), size=N)\n",
        "    MMSE = np.round(np.random.normal(mu_MMSE, sigma_MMSE, size=N))\n",
        "    EDUC = np.round(np.random.normal(mu_EDUC, sigma_EDUC, size=N))\n",
        "    Sex = np.random.binomial(1, 0.2, size=N)\n",
        "    eTIV = np.round(np.random.normal(mu_eTIV, sigma_eTIV, size=N))\n",
        "    ASF = np.random.normal(size=N)\n",
        "    nWBV = np.random.normal(size=N)\n",
        "    Visit = np.round(np.linspace(0, 4, num=m))\n",
        "\n",
        "    B = np.array([0.1, -0.1, 0.2, -0.2, 0.3, 0.3, 0.1, 0.1, 0.2, 0.1])\n",
        "    b = np.random.normal(0, 0.5, size=N)\n",
        "\n",
        "    for i in range(N):\n",
        "        for j in range(m):\n",
        "            eta[i, j] = B[0] + B[1] * Visit[j] + B[2] * Age[i] + B[3] * MMSE[i] + B[4] * EDUC[i] + B[5] * Sex[i] + \\\n",
        "                        B[6] * eTIV[i] + B[7] * ASF[i] + B[8] * nWBV[i] + b[i]\n",
        "            p = scipy.special.expit(eta[i, j])\n",
        "            Y[i, j] = np.random.binomial(1, p)\n",
        "\n",
        "   \n",
        "\n",
        "    mat = np.empty((N * m, 10))\n",
        "    p = 0\n",
        "    for i in range(N):\n",
        "        jk = 0\n",
        "        for jj in range(p, p + m):\n",
        "            mat[jj, 0] = i + 1\n",
        "            mat[jj, 1] = Visit[jk]\n",
        "            mat[jj, 2] = Age[i]\n",
        "            mat[jj, 3] = MMSE[i]\n",
        "            mat[jj, 4] = EDUC[i]\n",
        "            mat[jj, 5] = Sex[i]\n",
        "            mat[jj, 6] = eTIV[jk]\n",
        "            mat[jj, 7] = ASF[jk]\n",
        "            mat[jj, 8] = nWBV[jk]\n",
        "            mat[jj, 9] = Y[i, jk]\n",
        "            jk += 1\n",
        "        p += m\n",
        "\n",
        "    # Convert the `mat` array to a DataFrame\n",
        "    df_mat = pd.DataFrame(mat, columns=['ID', 'Visit', 'Age', 'MMSE', 'EDUC', 'Sex', 'eTIV', 'ASF', 'nWBV', 'Y'])\n",
        "    df = pd.get_dummies(df_mat, columns=[\"Visit\"], prefix=\"Visit\")\n",
        "    RANDOM_STATE = 13\n",
        "    splitter = GroupShuffleSplit(test_size=.20, n_splits=1, random_state=RANDOM_STATE)\n",
        "    split = splitter.split(df, groups=df['ID'])\n",
        "    train_indexes, test_indexes = next(split)\n",
        "\n",
        "    X_train = df.iloc[train_indexes]\n",
        "    X_test = df.iloc[test_indexes]\n",
        "    age_scaler = MinMaxScaler()\n",
        "    educ_scaler = MinMaxScaler()\n",
        "    mmse_scaler = MinMaxScaler()\n",
        "    etiv_scaler = MinMaxScaler()\n",
        "\n",
        "    age_scaler.fit(X_train[\"Age\"].to_numpy().reshape(-1, 1))\n",
        "    educ_scaler.fit(X_train[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
        "    mmse_scaler.fit(X_train[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
        "    etiv_scaler.fit(X_train[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
        "\n",
        "    X_train[\"Age\"] = age_scaler.transform(X_train[\"Age\"].to_numpy().reshape(-1, 1))\n",
        "    X_train[\"EDUC\"] = educ_scaler.transform(X_train[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
        "    X_train[\"MMSE\"] = mmse_scaler.transform(X_train[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
        "    X_train[\"eTIV\"] = etiv_scaler.transform(X_train[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
        "\n",
        "    X_test[\"Age\"] = age_scaler.transform(X_test[\"Age\"].to_numpy().reshape(-1, 1))\n",
        "    X_test[\"EDUC\"] = educ_scaler.transform(X_test[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
        "    X_test[\"MMSE\"] = mmse_scaler.transform(X_test[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
        "    X_test[\"eTIV\"] = mmse_scaler.transform(X_test[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
        "    visit_3 = df[df[\"Visit_4.0\"] == 1][[\"ID\", \"Y\"]]\n",
        "    y_train_final = []\n",
        "    for id in X_train[\"ID\"]:\n",
        "        y_train_final.append(visit_3[visit_3[\"ID\"] == id][\"Y\"].values[0])\n",
        "\n",
        "    y_test_final = []\n",
        "    for id in X_test[\"ID\"]:\n",
        "        y_test_final.append(visit_3[visit_3[\"ID\"] == id][\"Y\"].values[0])\n",
        "\n",
        "    y_train_super_final = pd.DataFrame({\n",
        "    \"Group\": y_train_final})\n",
        "    y_test_super_final = pd.DataFrame({\n",
        "    \"Group\": y_test_final})\n",
        "    X_test_final = X_test.drop([\"ID\"], axis = 1)\n",
        "    X_train_final = X_train.drop([\"ID\"], axis = 1)\n",
        "    rows_n = 5\n",
        "\n",
        "    X_train_super_final = np.reshape(X_train_final.to_numpy(),(X_train_final.shape[0]//5,X_train_final.shape[1],rows_n))\n",
        "    X_test_super_final = np.reshape(X_test_final.to_numpy(),(X_test_final.shape[0]//5,X_train_final.shape[1],rows_n))\n",
        "    gru_rnn = build_gru_rnn((X_train_super_final.shape[1],X_train_super_final.shape[2]))\n",
        "    gru_rnn.fit(X_train_super_final, y_train_super_final[0:int(len(X_train_super_final))], epochs = 100)\n",
        "    not_final_gru_rnn_prediction = gru_rnn.predict(X_test_super_final)\n",
        "    gru_rnn_prediction = np.where(not_final_gru_rnn_prediction > 0.5, 1, 0)\n",
        "    print(classification_report(y_test_super_final,gru_rnn_prediction, target_names= [\"Non-Demanted\", \"Demanted\"]))\n",
        "    gru_rnn_conf = confusion_matrix(y_test_super_final[0:int(len(lstm_rnn_prediction))],gru_rnn_prediction)\n",
        "    gru_rnn_plot_conf = ConfusionMatrixDisplay(gru_rnn_conf)\n",
        "    gru_rnn_plot_conf.plot()\n",
        "    TP=lstm_rnn_conf[1,1]\n",
        "    FP=lstm_rnn_conf[0,1]\n",
        "    TN=lstm_rnn_conf[0,0]\n",
        "    FN=lstm_rnn_conf[1,0]\n",
        "    accuracy=(TP+TN)/(TP+FP+FN+TN)\n",
        "    recall =TP/(TP+FN)\n",
        "    precision =TP/(TP+TN)\n",
        "    accuracy_list.append(accuracy)\n",
        "    recall_list.append(recall)\n",
        "    precision_list.append(precision)\n",
        "\n",
        "print(\"Accuracy List:\", accuracy_list)\n",
        "print(\"Recall List:\", recall_list)\n",
        "print(\"Precision List:\", precision_list)"
      ],
      "metadata": {
        "id": "jlug_x19Nqu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xVRIIAipNqyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ixECL4BfMkt0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}